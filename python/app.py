import os
import json
import re
import warnings
from pathlib import Path
from typing import Optional, List, Dict
from collections import Counter, defaultdict
import numpy as np
from dotenv import load_dotenv
from fastapi import FastAPI, Request , Body
from fastapi.responses import StreamingResponse , PlainTextResponse , HTMLResponse
import sys, time
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
import requests 
#########
from symspellpy import SymSpell
# from langchain.retrievers.multi_query import MultiQueryRetriever
import httpx
from difflib import SequenceMatcher
from llama_cpp import Llama 
from meeting import * 
from utils import is_brochure_request
import asyncio 
warnings.filterwarnings("ignore", category=FutureWarning)
load_dotenv()

# ========================================
# Paths and globals
# ========================================
DATA_DIR = "documents"
EMB_PATH = "Embeddings"
META_FILE = os.path.join(EMB_PATH, "meta.json")
DATA_FILE = "documents/data1.txt"
OUTPUT_DICT = "dictionary.txt"
CONTEXT_FILE = "documents/context.txt"
MAX_CONTEXT_TURNS = 4
REFERENCE_CLEANUP="documents/cleanup_reference.txt"

SESSION_STATE = defaultdict(dict)  # per-session state: {"active_society": str}



EMB_PATH = "Embeddings"
EMB_FILE = os.path.join(EMB_PATH, "embeddings.npy")   # numeric embeddings
TEXT_FILE = os.path.join(EMB_PATH, "texts.json")      # original document text
META_FILE = os.path.join(EMB_PATH, "meta.json")       # timestamp + info

# ========================================
# Utilities
# ========================================
def get_file_timestamp(path: str) -> float:
    return os.path.getmtime(path)

def embeddings_exist() -> bool:
    return (
        os.path.exists(os.path.join(EMB_PATH, "index.faiss"))
        and os.path.exists(os.path.join(EMB_PATH, "index.pkl"))
        and os.path.exists(META_FILE)
    )           

def save_vectorstore(vectorstore: FAISS, timestamp: float) -> None:
    os.makedirs(EMB_PATH, exist_ok=True)
    vectorstore.save_local(EMB_PATH)
    json.dump({"timestamp": timestamp}, open(META_FILE, "w"))

def load_existing_vectorstore(embeddings):
    print("üîÑ Loading existing FAISS vector store...")
    return FAISS.load_local(EMB_PATH, embeddings, allow_dangerous_deserialization=True)

def format_docs(docs) -> str:
    return "\n\n".join(doc.page_content for doc in docs)

def get_session_id(request: Request) -> str:
    return request.headers.get("x-session-id") or request.client.host or "anon"

# ========================================
# Society name detection and tagging
# ========================================
SOCIETY_KEYWORDS = (
    "society","apartment","apartments","residency","residences","heights",
    "homes","estate","OneWorld developers","Galaxy Developers","Sobha Limited","Panchshil Realty","Lodha Group","galaxy","apex"
)
NAME_RX = re.compile(
    r"\b([A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+){0,3})\s+(%s)\b" % "|".join(SOCIETY_KEYWORDS),
    re.IGNORECASE,
)

def extract_society_name(text: str) -> Optional[str]:
    m = NAME_RX.search(text)
    if not m:
        return None
    return (" ".join([m.group(1), m.group(2)])).strip().title()

def load_documents(file_path: str = DATA_FILE):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")

    loader = TextLoader(file_path, encoding="utf-8")
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)
    chunks = splitter.split_documents(documents)

    last_name = None
    for d in chunks:
        found = extract_society_name(d.page_content)
        if found:
            last_name = found
        d.metadata = d.metadata or {}
        d.metadata["society"] = last_name or "unknown"
    return chunks

# ========================================
# Embeddings, Vectorstore, Retriever
# ========================================
def create_embeddings():
    return HuggingFaceEmbeddings(
        model_name="models/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )
    


def build_vectorstore() -> FAISS:
    print("üî® Rebuilding embeddings & index...")
    docs = load_documents(DATA_FILE)
    embeddings = create_embeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)
    timestamp = get_file_timestamp(DATA_FILE)
    save_vectorstore(vectorstore, timestamp)
    return vectorstore

def get_vectorstore_and_retriever():
    embeddings = create_embeddings()
    current_timestamp = get_file_timestamp(DATA_FILE)

    if embeddings_exist():
        stored_meta = json.load(open(META_FILE))
        if current_timestamp == stored_meta.get("timestamp", 0):
            vs = load_existing_vectorstore(embeddings)
            print("‚úÖ Using cached vectorstore")
            return vs, vs.as_retriever(search_kwargs={"k": 12})
        print("‚ö†Ô∏è Data changed ‚Üí refreshing embedding store")

    vs = build_vectorstore()
    return vs, vs.as_retriever(search_kwargs={"k": 12})

# def create_multiquery_retriever(vectorstore, llm):
    
#     return MultiQueryRetriever.from_llm(
#         retriever=vectorstore.as_retriever(search_kwargs={"k": 8}),
#         llm=llm  # you already have an LLM function
#     )


vectorstore, base_retriever = get_vectorstore_and_retriever()


def collect_known_societies(vs: FAISS) -> set[str]:
    names = set()
    # access underlying dict
    for _id, doc in vs.docstore._dict.items():
        s = doc.metadata.get("society") if doc.metadata else None
        if s and s != "unknown":
            names.add(str(s))
    return names

KNOWN_SOCIETIES = collect_known_societies(vectorstore)

# ========================================
# Reranker
# ========================================
def create_reranker(embeddings, top_k: int = 3, threshold: float = 0.3) -> RunnableLambda:
    def _rerank(inputs):
        docs = inputs["docs"]
        query = inputs["question"]
        if not docs:
            return []

        q_vec = np.array(embeddings.embed_query(query))
        d_texts = [d.page_content for d in docs]
        d_vecs = [np.array(v) for v in embeddings.embed_documents(d_texts)]

        def cos(a, b):
            denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-8
            return float(np.dot(a, b) / denom)

        # Compute scores and pair with docs
        scored_docs = []
        for doc, d_vec in zip(docs, d_vecs):
            score = cos(q_vec, d_vec)
            if score >= threshold:
                scored_docs.append((doc, score))

        # Sort by score descending
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        # Return only docs (no scores), up to top_k
        return [doc for doc, _ in scored_docs[:top_k]]

    return RunnableLambda(_rerank)


# ========================================
# Dictionary and spell correction
# ========================================
STOPWORDS = {
    "the","is","and","or","of","to","a","in","on","for","with","at",
    "are","you","we","it","do","have","what","how","i"
}
def build_dictionary():
    if not os.path.exists(DATA_FILE):
        raise FileNotFoundError(f"{DATA_FILE} not found!")

    with open(DATA_FILE, "r", encoding="utf-8") as f:
        content = f.read().lower()

    words = re.findall(r"[a-zA-Z0-9]+", content)
    filtered = [w for w in words if w not in STOPWORDS and len(w) > 2]
    word_counts = Counter(filtered)

    with open(OUTPUT_DICT, "w", encoding="utf-8") as f:
        for word, count in word_counts.items():
            f.write(f"{word} {count}\n")

    print(f"‚úÖ Dictionary created: {OUTPUT_DICT}")
    print(f"üìå Total unique words: {len(word_counts)}")

sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)
if os.path.exists(OUTPUT_DICT):
    sym_spell.load_dictionary(OUTPUT_DICT, term_index=0, count_index=1)
else:
    # safe load if not yet built; will rebuild on first query
    pass

def correct_spelling(text: str) -> str:
    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)
    return suggestions[0].term if suggestions else text

# ========================================
# Intent detection
# ========================================
def find_brochure_file():
    if not os.path.exists(DATA_DIR):
        return None
    brochure_patterns = [
        r".*brochure.*\.(pdf|PDF)",
        r".*catalog.*\.(pdf|PDF)",
        r".*info.*\.(pdf|PDF)",
        r".*details.*\.(pdf|PDF)",
    ]
    for filename in os.listdir(DATA_DIR):
        for pattern in brochure_patterns:
            if re.search(pattern, filename, re.IGNORECASE):
                return os.path.join(DATA_DIR, filename)
    return None

def detect_special_intent(query: str):
    q = query.lower().strip()
    if re.search(r"\b(brochure|brouchure|catalog|pdf|info sheet)\b", q):
        path = find_brochure_file()
        return {"type": "brochure", "content": path}
    meeting_text = "Call to Mr. akash on +91 8208333212 and Schedule a meeting.."
    if re.search(r"\b(schedule|book|set up|arrange).*?(call|meeting|appointment|demo)\b", q):
        return {"type": "schedule", "content": meeting_text}
    return None

def detect_society_in_query(q: str, known_names: set[str]) -> Optional[str]:
    m = NAME_RX.search(q)
    if m:
        return (" ".join([m.group(1), m.group(2)])).strip().title()
    q_low = q.lower()
    for name in sorted(known_names, key=len, reverse=True):
        if name.lower() in q_low:
            return name
    return None

def majority_society(docs) -> Optional[str]:
    vals = [d.metadata.get("society","unknown") for d in docs if getattr(d, "metadata", None)]
    if not vals:
        return None
    c = Counter(vals)
    top, _ = c.most_common(1)[0]
    return None if top.lower() == "unknown" else top

class SocietyFilteredRetriever:
    def __init__(self, base_retriever, society: Optional[str], k=8):
        self.base = base_retriever
        self.society = society
        self.k = k

    def invoke(self, query: str):
        docs = self.base.invoke(query)
        if not self.society:
            return docs[: self.k]
        filtered = [d for d in docs if (d.metadata or {}).get("society","").lower() == self.society.lower()]
        if len(filtered) >= max(3, self.k // 2):
            return filtered[: self.k]
        need = self.k - len(filtered)
        extras = [d for d in docs if d not in filtered]
        return (filtered + extras[:need])[: self.k]

# ========================================
# Chat history: fetch, select, format
# ========================================


# ‚úÖ Modified history fetcher (sync call allowed inside async)
def fetch_chat_history(title_id, max_pairs=4):  # ‚úÖ set number of turns you want
    if not title_id:
        return []

    try:
        url = f"http://3.6.203.180:7601/chatbot/getAllChats?title_id={title_id}"
        res = requests.get(url, timeout=5)
        res.raise_for_status()
        data = res.json()

        chat_pairs = []
        temp_user_msg = None

        if "data" in data:
            for entry in data["data"]: 
                print(entry)
                sender = entry.get("sender", "").strip().lower()
                message = entry.get("message", "").strip()

                if not message:
                    continue

                if sender == "user":
                    # ‚úÖ Start new pair if a user message arrives
                    temp_user_msg = message

                elif sender == "bot":
                    # ‚úÖ If bot replies, pair with last user message
                    chat_pairs.append({
                        "user": temp_user_msg or "",
                        "response": message
                    })
                    temp_user_msg = None

        # ‚úÖ Return last N conversation turns (not only one)
        return chat_pairs[-max_pairs:]

    except Exception as e:
        print(f"[history] fetch failed: {e}")
        return []

def fetch_chat_history_local(title_id): 
    try:
        url = f"http://localhost:4001/history"
        res = requests.get(url, timeout=5)
        res.raise_for_status()
        data = res.json()

        chat_pairs = []
        temp_user_msg = None

        if "data" in data:
            for entry in data["data"]: 
                sender = entry.get("sender", "").strip().lower()
                message = entry.get("message", "").strip()

                if not message:
                    continue

                if sender == "user":
                    # ‚úÖ Start new pair if a user message arrives
                    temp_user_msg = message

                elif sender == "bot":
                    # ‚úÖ If bot replies, pair with last user message
                    chat_pairs.append({
                        "user": temp_user_msg or "",
                        "response": message
                    })
                    temp_user_msg = None

        # ‚úÖ Return last N conversation turns (not only one)
        return chat_pairs[-6:]

    except Exception as e:
        print(f"[history] fetch failed: {e}")
        return []


def select_relevant_history(
    query: str,
    history: List[Dict[str, str]],
    embeddings,
    k_similar: int = 4,
    last_n: int = 4,
    max_chars: int = 1800,
) -> List[Dict[str, str]]:
    """
    Keep a small working memory:
      - last_n most recent turns
      - top k_similar semantically close older turns
      - truncate to max_chars after formatting
    """
    if not history:
        return []

    # Separate recent and older
    recent = history[-last_n:] if last_n > 0 else []
    older = history[:-last_n] if last_n > 0 else history

    # Semantic pick from older
    picks = []
    if older and k_similar > 0:
        q_vec = embeddings.embed_query(query)
        def pair_text(p): return f"User: {p.get('user','')}\nAssistant: {p.get('response','')}"
        older_texts = [pair_text(p) for p in older]
        older_vecs = embeddings.embed_documents(older_texts)

        def cos(a, b):
            a = np.array(a); b = np.array(b)
            denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-8
            return float(np.dot(a, b) / denom)

        scores = [cos(q_vec, v) for v in older_vecs]
        idx_sorted = np.argsort(scores)[::-1][:k_similar]
        picks = [older[i] for i in idx_sorted]

    # Combine while preserving order for recent; picks keep their relative order by original index
    combined = picks + recent

    # Trim by character budget
    def fmt(hist):
        lines = []
        for h in hist:
            u = h.get("user","").strip()
            a = h.get("response","").strip()
            if u:
                lines.append(f"User: {u}")
            if a:
                lines.append(f"Assistant: {a}")
            lines.append("")  # blank line
        return "\n".join(lines).strip()

    # If too long, drop oldest from picks first
    while combined and len(fmt(combined)) > max_chars:
        if picks:
            picks.pop(0)
            combined = picks + recent
        elif recent:
            recent.pop(0)
            combined = picks + recent
        else:
            break
    return combined

def format_history_for_prompt(hist: List[Dict[str, str]]) -> str:
    if not hist:
        return ""
    parts = []
    for h in hist:
        u = h.get("user","").strip()
        a = h.get("response","").strip()
        if u:
            parts.append(f"User: {u}")
        if a:
            parts.append(f"Assistant: {a}")
        parts.append("")
    return "\n".join(parts).strip()

# ================================================================================= 
#      Saving Context in context.txt 

def load_context_file():
    if not os.path.exists(CONTEXT_FILE):
        return []
    try:
        with open(CONTEXT_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data if isinstance(data, list) else []
    except:
        return []

def append_context_to_file(new_pair: dict):
    context = load_context_file()
    context.append(new_pair)
    if len(context) > 2:
        context = context[-2:]
        # print(context)
    with open(CONTEXT_FILE, "w", encoding="utf-8") as f:
        json.dump(context, f, indent=2) 


# ==================================================================================
# removing the context when new chat start 
LAST_TITLE_ID = None
def reset_context():
    global LAST_TITLE_ID
    LAST_TITLE_ID = None
    if os.path.exists(CONTEXT_FILE):
        with open(CONTEXT_FILE, "w", encoding="utf-8") as f:
            f.write("[]")
    SESSION_STATE.clear()
    print("‚úÖ Context reset due to new title_id")

# ========================================
# Prompt and LLM
# ========================================
def create_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "You are a polite, concise, highly accurate real estate assistant named Arya.\n"
         "Rules:\n"
         "‚Ä¢ If the user greets (hi/hello/hey/good morning/etc.), respond with a short friendly greeting, then ask how you can help with real estate.\n"
         "‚Ä¢ Otherwise, DO NOT greet repeatedly. Answer the query directly and move the conversation forward.\n"
         "‚Ä¢ Use ONLY the information in Knowledge and the limited Chat History for continuity.\n"
         "‚Ä¢ If Knowledge is empty or insufficient for the request, ask a SPECIFIC follow-up to gather exactly what‚Äôs missing (e.g., location, BHK, budget, possession timeline) ‚Äî do not repeat the same generic question.\n"
         "‚Ä¢ If the question is unrelated to real estate, reply: 'I can only help with real estate-related questions.'\n"
         "‚Ä¢ Do not mention 'Knowledge', 'documents', or 'sources'.\n"
         "‚Ä¢ For direct factual requests, answer directly using Knowledge.\n"
         "‚Ä¢ Keep responses SHORT and CLEAR. Always respond in English.\n\n"
         "Active Society: {active_society}\n\n"
         "Chat History (selected):\n{chat_history}\n\n"
         "Knowledge:\n{context}"
        ),
        ("human", "{question}")
    ])


import re

def build_reference_set(text: str):
    """
    Extracts all unique numeric patterns and key property terms 
    from the given text for reference correction.
    """
    numbers = re.findall(r'\b\d+(?:[.,]\d+)?\b', text)
    words = re.findall(r'\b[A-Za-z]{3,}\b', text)
    reference_set = set(numbers + words)
    return reference_set

def smart_fix_spaces_dynamic(text: str, reference_words: set = None) -> str:
    # --- 1Ô∏è‚É£ Fix numeric spacing (numbers, decimals, ranges, times) ---
    text = re.sub(r'(?<=\d)\s+(?=\d)', '', text)           # 1 0 ‚Üí 10
    text = re.sub(r'(\d)\s*:\s*(\d)', r'\1:\2', text)      # 10 : 90 ‚Üí 10:90
    text = re.sub(r'(\d)\s*\.\s*(\d)', r'\1.\2', text)     # ‚Çπ1 . 5 ‚Üí ‚Çπ1.5
    text = re.sub(r'(\d)\s*-\s*(\d)', r'\1‚Äì\2', text)      # 850 - 1100 ‚Üí 850‚Äì1100 (en dash)
    text = re.sub(r'([‚Çπ])\s*([0-9])', r'\1\2', text)       # ‚Çπ 95 ‚Üí ‚Çπ95
    text = re.sub(r'(\d)(\s*)(lakhs|crores)', r'\1 \3', text, flags=re.IGNORECASE)

    # --- 2Ô∏è‚É£ Fix common real-estate patterns ---
    text = re.sub(r'(\d)\s*B\s*H\s*K', r'\1BHK', text, flags=re.IGNORECASE)
    text = re.sub(r'(\d)\s*BHK', r'\1BHK', text, flags=re.IGNORECASE)
    text = re.sub(r'\bQ\s*([1-4])\s*([0-9]{4})\b', r'Q\1 \2', text)  # Q 42026 ‚Üí Q4 2026
    text = re.sub(r'\b(\d+)\s*(sq\s*ft|SQ\s*FT|Sq\s*Ft)\b', r'\1 sq ft', text, flags=re.IGNORECASE)

    # --- 3Ô∏è‚É£ Fix names like "Mr. Ak ash" ---
    text = re.sub(r'\b(Mr|Ms|Mrs|Dr)\.\s+([A-Z])\s+([a-z]+)', r'\1. \2\3', text)

    # --- 4Ô∏è‚É£ Fix URL / domain spacing ---
    text = re.sub(r'https\s*:\s*/\s*/\s*', 'https://', text)
    text = re.sub(r'www\s*\.\s*', 'www.', text)
    text = re.sub(r'\s*\.\s*com', '.com', text)
    text = re.sub(r'\s*\.\s*in', '.in', text)
    text = re.sub(r'\s*\.\s*org', '.org', text)
    text = re.sub(r'\s*\.\s*net', '.net', text)

    # --- 5Ô∏è‚É£ Context-based joining (reliable only for long words) ---
    if reference_words:
        for word in sorted(reference_words, key=len, reverse=True):
            if len(word) >= 4:
                pattern = r'\b' + r'\s*'.join(list(word)) + r'\b'
                text = re.sub(pattern, word, text, flags=re.IGNORECASE)

    # --- 6Ô∏è‚É£ General punctuation & space cleanup ---
    text = re.sub(r'\s+([.,!?;:])', r'\1', text)  # no space before punctuation
    text = re.sub(r'([.,!?;:])([A-Za-z0-9])', r'\1 \2', text)  # ensure space after punctuation
    text = re.sub(r'\s{2,}', ' ', text)  # collapse multiple spaces
    text = text.strip()

    return text


# ========================================================
# High hello structured 


def normalize_text(text: str) -> str:
    """Normalize text for better fuzzy matching"""
    text = text.lower().strip()
    text = re.sub(r'(.)\1{2,}', r'\1', text)  # hiiii -> hi
    text = re.sub(r'[^a-z\s]', '', text)      # remove punctuation
    return text

def similarity(a: str, b: str) -> float:
    """Compute similarity ratio"""
    return SequenceMatcher(None, a, b).ratio()

def detect_greeting(text: str):
    """Return greeting type and whether it's a standalone greeting"""
    responses_map = {
        "hello": "Hello! How i can Help you?",
        "hi": "Hi there! How i can help you?",
        "hey": "Hey! I'm here",
        "greetings": "Greetings! How may I be of service?",
        "good morning": "Good morning! How can I help you today?",
        "good afternoon": "Good afternoon! What would you like to know?",
        "good evening": "Good evening! How may I assist you?",
        "good day": "Good day to you! How can I help?",
        "what's up": "Not much, just ready to help you! What do you need?",
        "how are you": "I'm doing well, thank you! How can I help you today?",
        "morning": "Good morning! How can I assist you today?",
        "afternoon": "Good afternoon! How may I help you?",
        "evening": "Good evening! What can I do for you?",
        "good night": "Good night! Is there anything I can help you with before you go?",
        "howdy": "Howdy! What can I help you with today?",
        "yo": "Hello! How may I assist you?",
        "hi there": "Hi there! How can I help you?",
        "hello there": "Hello there! What would you like to know?",
        "good to see you": "Good to see you too! How can I assist you today?",
        "nice to see you": "Nice to see you as well! What can I help you with?",
        "long time no see": "Hello! It's good to connect with you again. How may I help?",
        "what's new": "Nothing much here! How can I assist you today?",
        "how's it going": "Things are going well! How may I help you?",
        "how have you been": "I've been doing great, thank you for asking! How can I assist you?",
        "hope you're well": "Thank you for asking! I'm here and ready to help. What do you need?",
        "good to meet you": "Good to meet you too! How may I be of service?",
        "pleased to meet you": "Pleased to meet you as well! What can I help you with today?",
        "welcome": "Thank you! How can I assist you?",
        "hey there": "Hey there! I'm ready to help. What do you need?",
        "what's happening": "Not much on my end! How can I help you today?",
        "namaste": "Namaste! How may I be of service?",
       "thanks": "You're welcome! Happy to help! üòä",
    "thank you": "Thank you! I'm glad I could assist you!",
   
    }

    normalized = normalize_text(text)
    best_match = None
    best_score = 0.0

    for key_greeting, response_message in responses_map.items():
        score = similarity(normalized, key_greeting)
        if score > best_score:
            best_score = score
            best_match = (key_greeting, response_message)

    # Detect if greeting word exists in start of message
    greeting_found = any(key in normalized.split()[:3] for key in responses_map)

    # 1Ô∏è‚É£ If the message is *only* a greeting ‚Üí respond immediately
    if len(normalized.split()) <= 3 and (best_score >= 0.6 or greeting_found):
        return {"is_greeting": True, "response": best_match[1]}

    # 2Ô∏è‚É£ If greeting present + other intent words ‚Üí skip greeting
    if greeting_found or best_score >= 0.6:
        return {"is_greeting": False, "response": None}

    # 3Ô∏è‚É£ Not a greeting at all
    return {"is_greeting": False, "response": None}
 
#=========================================================
def create_llm():

    llm = Llama( 
        model_path='models/qwen2.5-3b-instruct-q4_k_m.gguf',
        # model_path='models/qwen2.5-1.5b-instruct-q4_k_m.gguf',
        n_ctx=2048,
        n_threads=8,
        n_batch=1024 , 
        verbose=False
    )
    return llm

embeddings = create_embeddings()
rerank = create_reranker(embeddings, top_k=5)
prompt = create_prompt()  
global_llm  = create_llm() 

parser = StrOutputParser() 


meeting_bot = MeetingSchedulerBot()
MEETING_SESSION = {}
# ========================================
# FastAPI
# ========================================
app = FastAPI(title="Real Estate Chatbot")

@app.api_route("/stream_info", methods=["POST"])
async def ask_chat(request: Request ,  body: dict = Body(None)):
    global LAST_TITLE_ID

    title_id = None
    query = None
    user_id = None

    # If POST ‚Üí get title_id + query from body
    if request.method == "POST":
        title_id = (body or {}).get("title_id")
        query = (body or {}).get("query") 
        user_id = (body or {}).get("user_id") 

        if title_id and title_id != LAST_TITLE_ID:
            reset_context()
            LAST_TITLE_ID = title_id 
    
        if user_id: 
            print("user_id mil gaya")
    session_id = get_session_id(request)
    

    if not query or not query.strip():
        return "Please enter a query."
        
    if is_brochure_request(query):
        brochure_path = "http://3.6.203.180:7602/documents/Brochure.pdf"
        return HTMLResponse(
    f'Here is your brochure: {brochure_path}'
)
        
    # ‚úÖ INSERT GREETING HANDLER HERE
    greeting_check = detect_greeting(query)
    if greeting_check["is_greeting"] and greeting_check["response"]:
        async def greeting_stream():
            for ch in greeting_check["response"]:
                yield ch
                await asyncio.sleep(0.05)
        return StreamingResponse(greeting_stream(), media_type="text/plain; charset=utf-8")

    if not greeting_check["is_greeting"] and greeting_check["response"] is None:
        for key in ["hi", "hello", "hey", "good morning", "good afternoon", "good evening", "namaste", "greetings"]:
            if query.lower().startswith(key):
                query = query[len(key):].strip(",.! ").strip()
                break

    
    # Detect society and manage session
    mentioned = detect_society_in_query(query, KNOWN_SOCIETIES)
    if mentioned:
        SESSION_STATE[session_id]["active_society"] = mentioned
    if re.search(r"\b(reset|clear)\b.*\bsociety\b", query, re.I):
        SESSION_STATE[session_id]["active_society"] = None

    active_society = SESSION_STATE[session_id].get("active_society")

    # Retrieve relevant context
    retriever = SocietyFilteredRetriever(base_retriever, active_society, k=8)
    docs = retriever.invoke(query)
    ranked_docs = rerank.invoke({"docs": docs, "question": query})
    context = format_docs(ranked_docs) 
    
    if len(context) > 2000:
        context = context[:2000]
        
    ### Main LLM Call  
    llm = global_llm 

    # Load local and external chat history
    if title_id!=None  and title_id!= 0: 
        full_history = fetch_chat_history(title_id) 
    else: 
        full_history = ""
    local_context = load_context_file()
    combined_history = (full_history or []) + local_context
    local_context = load_context_file()  # keep as text

    selected_hist = select_relevant_history(
        query,
        combined_history,
        embeddings,
        k_similar=4,
        last_n=4,
        max_chars=2000,
    ) or []

    new_turn = {"user": query, "response": ""}
    selected_hist.append(new_turn)
    chat_history_text = format_history_for_prompt(selected_hist) 
    if len(chat_history_text)> 800: 
        chat_history_text = chat_history_text[-800:]
  
    if session_id not in MEETING_SESSION:
        MEETING_SESSION[session_id] = MeetingSchedulerBot()

    scheduler = MEETING_SESSION[session_id]
    scheduler.user_id = user_id

    # If we are currently expecting date or purpose ‚Üí ALWAYS process here
    if scheduler.awaiting in ["datetime", "purpose"]:
        reply = scheduler.respond(query, chat_history=chat_history_text.split("\n"))
        return PlainTextResponse(reply)

    # If user STARTS a meeting request
    if is_meeting_request(query):
        scheduler.reset_state()
        reply = scheduler.respond(query)
        return PlainTextResponse(reply) 
    
    last_char = ""  
    def cleaner(text):
        return re.sub(r'(?<=[A-Za-z])(?=\d)|(?<=\d)(?=[A-Za-z])', ' ', text)
    if len(context.strip())< 10: 
        print("no Context")
        system_prompt = (
        "You are Arya ‚Äî a warm, polite, and expert real-estate assistant. "
        "If the user greets you (hi, hello, hey, good morning, namaste, good evening etc.), "
        "respond with a friendly, short greeting and add ONE polite follow-up question such as "
        "'How can I help you today?' or 'Would you like details about any project?'. "
        "Your single source of truth is the section called 'Knowledge'. "
        "Treat the Knowledge content as verified, up-to-date, and directly relevant to the user's query. "
        "you must answer using that information directly and confidently. "
        "Do not ask for the project or developer again ‚Äî use what is provided in Knowledge. "
        "Don't Use Certainly word in response"
        "Your tone should be empathetic, natural, and professional ‚Äî like a helpful real estate consultant. "
        "Avoid generic responses or repeating the user's query. "
        "Be concise, accurate, and factual."
    )
        chatml_prompt = f"""
    <|system|>
    {system_prompt}
    <|end|>
    <|user|>

    Chat History:
    {chat_history_text}

    The following Knowledge is guaranteed to be relevant to this user's query ‚Äî it has been carefully retrieved from verified real estate data. Use it to answer directly.

    Knowledge:
    {context}

    User Query:
    {query}
    <|end|>
    <|assistant|>
    """ 
        async def stream_response():
            response_text = ""
            prev_chunk = ""
            buffer = ""  
            word_buffer = []
            start_time = time.time()
            print("üéàüéàStart generating response ......")
            for token in llm(
                chatml_prompt,
                max_tokens=200,  # shorter and safer
                temperature=0.25,
                top_p=0.85,
                repeat_penalty=1.05,
                stream=True,
                presence_penalty=0.3,
                stop=['<|end|>',"<|user|>", "<|system|>", "\n\n\n"],
            ):
                chunk = token["choices"][0].get("text", "")
                if chunk.strip() == "":
                    continue 
                
                if not chunk:
                    continue

                # Prevent duplication
                if chunk.strip() == "" or chunk.strip() == prev_chunk.strip():
                    continue  
                
                if len(response_text) > 800:
                    print("\n[üõë Auto-stop after 800 chars]\n")
                    break 
                
                buffer += chunk

            # if chunk ends with space or punctuation ‚Üí treat buffer as a full word
            if buffer.endswith((" ", ".", ",", "!", "?", ":", ";")):
                cleaned = cleaner(buffer)
                yield cleaned
                response_text += cleaned
                buffer = ""  

            await asyncio.sleep(0)
        return StreamingResponse(
            stream_response(),
            media_type="text/plain",
            headers={"Transfer-Encoding": "chunked"}
        )
                
        
    if  len(context)>10: 
        append_context_to_file({"user": query, "response": context}) 
 

    # üß© Clean system message for DeepSeek model
    

    system_prompt = ( 
    "You are Arya ‚Äî a warm, polite, and expert real-estate assistant. "
    "Your single source of truth is the section called 'Knowledge'. " 
    "Treat the Knowledge content as verified, up-to-date, and directly relevant to the user's query. "
    "If the Knowledge includes any details about the user‚Äôs question (e.g., price, area, project name, BHK type, developer), " 
    "you must answer using that information directly and confidently. "
    "‚ö†Ô∏è Preserve all numbers *exactly as written in the Knowledge section*, including zeros and commas (e.g., 1000, 25000, 3.50). Never round, truncate, or reformat them. "
    "Do not ask for the project or developer again ‚Äî use what is provided in Knowledge. "
    "Only if Knowledge is completely empty should you ask a follow-up. " "Don't Use Certainly, first Conversation in response. "
    "Your tone should be empathetic, natural, and professional ‚Äî like a helpful real estate consultant. " 
    "Avoid generic responses or repeating the user's query. " "Be concise, accurate, and factual." )

    chatml_prompt = f"""
<|system|>
{system_prompt}
<|end|>
<|user|>
Active Society: {active_society or "None"}

Chat History:
{chat_history_text}

The following Knowledge is guaranteed to be relevant to this user's query ‚Äî it has been carefully retrieved from verified real estate data. Use it to answer directly.

Knowledge:
{context}

User Query:
{query}
<|end|>
<|assistant|>
""" 


    complete_context_reference_set = chat_history_text +"\n" + context
    reference_words = build_reference_set(complete_context_reference_set)  
    last_char = ""

    async def stream_response(): 
        print('if context')
        start_time = time.time()
        global last_char
        last_char = ""

        response_text = ""   # full accumulated output
        buffer = ""          # word reconstruction buffer
       # word reconstruction buffer
        prev_chunk = "" 
        word_buffer = []
        print("üéàüéàStart generating response ......")
        for token in llm(
            chatml_prompt,
            max_tokens=400,  # shorter and safer
            temperature=0.25,
            top_p=0.85,
            repeat_penalty=1.05,
            stream=True,
            presence_penalty=0.3,
            stop=['<|end|>',"<|user|>", "<|system|>", "\n\n\n"],
        ):
            chunk = token["choices"][0].get("text", "")
            
            if not chunk.strip():
                continue

            # accumulate raw chunk first
            buffer += chunk

            # if chunk ends with space or punctuation ‚Üí treat buffer as a full word
            if buffer.endswith((" ", ".", ",", "!", "?", ":", ";")):
                cleaned = cleaner(buffer)
                yield cleaned
                response_text += cleaned
                buffer = ""  # reset buffer

            await asyncio.sleep(0)

        # flush remaining
        if buffer:
            cleaned = cleaner(buffer)
            yield cleaned
            response_text += cleaned
            
    return StreamingResponse(
    stream_response(),
    media_type="text/plain",
    headers={"Transfer-Encoding": "chunked"}
)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7602, log_level="info")
